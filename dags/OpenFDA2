# dags/openfda_cosmetic_events_to_bigquery_v2.py

from __future__ import annotations

import pandas as pd
import pendulum
import requests
from airflow.decorators import dag, task
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

# ========================= CONFIGURAÇÕES =========================
GCP_PROJECT = "bigqueryenap"  # ⚠️ Verifique se o nome do projeto está correto
BQ_DATASET = "FDA"            # ⚠️ Verifique se o nome do dataset está correto
BQ_TABLE = "cosmetic_events_weekly"
GCP_CONN_ID = "google_cloud_default"
# =================================================================


@dag(
    dag_id="openfda_cosmetic_events_to_bigquery_v2",
    description="ETL idempotente de eventos de cosméticos do OpenFDA para o BigQuery.",
    schedule="@monthly",
    start_date=pendulum.datetime(2023, 1, 1, tz="UTC"),
    catchup=True,
    tags=["openfda", "cosmetics", "bigquery", "v2"],
    max_active_runs=1,
)
def cosmetic_events_etl_dag_v2():
    """
    ### DAG de ETL para Eventos Adversos de Cosméticos (Versão 2 - Idempotente)

    Este pipeline realiza as seguintes etapas:
    1.  **Extrai e Transforma**: Busca dados mensais da API openFDA e os agrega por semana.
    2.  **Carrega**: Garante que os dados do período sejam limpos da tabela de destino
        antes de inserir os novos, evitando duplicatas.
    """

    @task
    def fetch_and_transform_data(data_interval_start: pendulum.DateTime) -> pd.DataFrame:
        year = data_interval_start.year
        month = data_interval_start.month
        start_date_str = f"{year}{month:02d}01"
        last_day = data_interval_start.end_of("month").day
        end_date_str = f"{year}{month:02d}{last_day}"

        print(f"Buscando dados para o período de {start_date_str} a {end_date_str}")
        api_url = (
            "https://api.fda.gov/cosmetic/event.json?"
            f"search=initial_received_date:[{start_date_str}+TO+{end_date_str}]"
            "&count=initial_received_date"
        )
        response = requests.get(api_url, timeout=30)
        response.raise_for_status()
        data = response.json()
        df = pd.DataFrame(data.get("results", []))
        if df.empty:
            print("Nenhum dado encontrado para o período.")
            return pd.DataFrame()
        df['time'] = pd.to_datetime(df['time'], format="%Y%m%d")
        weekly_sum = df.groupby(pd.Grouper(key='time', freq='W-SUN'))['count'].sum().reset_index()
        weekly_sum.rename(columns={'time': 'week_start_date', 'count': 'total_events'}, inplace=True)
        print("Dados agregados por semana:")
        print(weekly_sum.head())
        return weekly_sum

    @task
    def load_to_bigquery(df: pd.DataFrame):
        """
        Salva o DataFrame no BigQuery de forma idempotente.
        Primeiro, deleta os dados do período correspondente e depois insere os novos.
        """
        if df.empty:
            print("DataFrame vazio. Nenhuma ação de carga será executada.")
            return

        df['week_start_date'] = pd.to_datetime(df['week_start_date']).dt.date
        
        # Pega a data mínima e máxima do DataFrame para definir o período a ser limpo
        min_date = df['week_start_date'].min().strftime('%Y-%m-%d')
        max_date = df['week_start_date'].max().strftime('%Y-%m-%d')

        print(f"Limpando dados na tabela de destino entre {min_date} e {max_date}...")

        destination_table = f"{GCP_PROJECT}.{BQ_DATASET}.{BQ_TABLE}"
        bq_hook = BigQueryHook(gcp_conn_id=GCP_CONN_ID, use_legacy_sql=False)

        # SQL para deletar os registros no intervalo de datas do DataFrame atual
        delete_sql = (
            f"DELETE FROM `{destination_table}` "
            f"WHERE week_start_date >= '{min_date}' AND week_start_date <= '{max_date}'"
        )
        
        # Executa a query de deleção
        bq_hook.run_query(sql=delete_sql)
        
        print("Dados antigos do período foram limpos. Carregando novos dados...")

        table_schema = [
            {"name": "week_start_date", "type": "DATE"},
            {"name": "total_events", "type": "INTEGER"},
        ]
        
        df.to_gbq(
            destination_table=destination_table,
            project_id=GCP_PROJECT,
            credentials=bq_hook.get_credentials(),
            if_exists="append", # Agora 'append' é seguro, pois limpamos a janela de dados
            table_schema=table_schema,
            progress_bar=False,
        )
        print(f"Carregados {len(df)} registros para a tabela: {destination_table}.")

    # Define o fluxo de tarefas
    weekly_data_df = fetch_and_transform_data(data_interval_start="{{ data_interval_start }}")
    load_to_bigquery(weekly_data_df)

# Instancia a DAG
cosmetic_events_etl_dag_v2()
